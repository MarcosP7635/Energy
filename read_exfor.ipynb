{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Cross-Section-Data\\\\EXFOR\\\\\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Unitful #https://painterqubits.github.io/Unitful.jl/stable/\n",
    "#quantity * @u_str(\"unit abbreviation\") \n",
    "using Symbolics #https://symbolics.juliasymbolics.org/dev/\n",
    "#cite https://doi.org/10.48550/arXiv.2105.03949\n",
    "using Latexify\n",
    "using Test\n",
    "#1 * @u_str(\"mA\") is 1 milliamp\n",
    "using CSV, DataFrames\n",
    "#using Plots\n",
    "using PlotlyJS\n",
    "using Printf\n",
    "using SymPy\n",
    "using PDFIO\n",
    "using Unzip\n",
    "using Interpolations\n",
    "using Plots\n",
    "using TensorCast\n",
    "using CatViews\n",
    "using MappedArrays\n",
    "#plotlyjs()\n",
    "data_dir = \"C:\\\\Cross-Section-Data\\\\EXFOR\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Design\n",
    "\n",
    "    Looping check_line() through each line in the file detects where each dataset begins. \n",
    "\n",
    "    Reading the line above each output of check_line() gives the number of rows in each dataset.\n",
    "\n",
    "    make_spacing_dict() gets the names of the columns and the instructions for how to read each dataset.\n",
    "\n",
    "    read_dataset() reads each and constructs a dictionary from each dataset.\n",
    "    \n",
    "    read_exfor_file() runs all of the above in order to return a single DataFrame of all the data stored at a given file path.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_dataset (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function check_line(line, start)\n",
    "    if length(line) > length(start) - 1\n",
    "        return (line[1:length(start)] == start)\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "function make_spacing_dict(line1, line2)\n",
    "    spacing_ends = [collect(out)[1] for out in findall(\">\", line2)]\n",
    "    spacing_starts = append!([1], [collect(out)[1] for out in findall(\"<\", line2)])\n",
    "    #Find where some of the spacings begin and end based on the arrows\n",
    "    if length(spacing_ends) != length(spacing_starts)\n",
    "        print(\"Error: improper formatting\")\n",
    "        return \n",
    "    end\n",
    "    indices_with_gaps = [index for index in  1:length(spacing_ends)-1 if \n",
    "                    spacing_ends[index] != spacing_starts[index+1]-1]\n",
    "    #Some of the spacings are instead denoted by the letter o instead of arrows\n",
    "    missing_spacings = [spacing_ends[index]+1:spacing_starts[index+1]-1 for index in indices_with_gaps]\n",
    "    spacings = append!(missing_spacings, \n",
    "    [spacing_starts[i]:spacing_ends[i] for i in 1:length(spacing_ends)])\n",
    "    spacing_dict = Dict([])\n",
    "    #Make a dictionary where the keys are name for each column and the values are the indices of the columns\n",
    "    spacing_names = [spacing_dict[strip(line1[spacing], [' ', '#'])] = spacing for spacing in spacings]\n",
    "    return spacing_dict\n",
    "end\n",
    "\n",
    "function read_datum(datum)\n",
    "    #Reads a single datum from a line of data\n",
    "    datum = strip(datum, [' '])\n",
    "    out = tryparse(Float64, datum)\n",
    "    if out == nothing\n",
    "        return datum \n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "function read_dataset(spacing_specifier, file_as_vector)\n",
    "    index = spacing_specifier\n",
    "    spacing_dict = make_spacing_dict(file_as_vector[index], file_as_vector[index + 1])\n",
    "    lines_of_data = tryparse(Int64, split(file_as_vector[index - 1], [' '])[end])\n",
    "    data_dict = Dict([])\n",
    "    filler = [[read_datum(file_as_vector[line_num][spacing_dict[key]])\n",
    "                for line_num in index + 2: index + 1 + lines_of_data] \n",
    "                for key in keys(spacing_dict)]\n",
    "    return filler#, lines_of_data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check_spacings (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function check_spacings(file_path)\n",
    "    #Reads an EXFOR file and returns a dictionary of data\n",
    "    file_as_vector = readlines(file_path)\n",
    "    spacing_specifiers = [index for index in 1:length(file_as_vector) \n",
    "                        if check_line(file_as_vector[index], \"# Prj\")]\n",
    "    if spacing_specifiers == []\n",
    "        print(\"Error: no data found\")\n",
    "        return\n",
    "    end\n",
    "    list_spacing_dict = [make_spacing_dict(file_as_vector[spacing_specifier], \n",
    "                                            file_as_vector[spacing_specifier+1])\n",
    "                        for spacing_specifier in spacing_specifiers]\n",
    "    #check if the spacing dicts are the same\n",
    "    for index in 2:length(spacing_specifiers)\n",
    "        if list_spacing_dict[index] != list_spacing_dict[1]\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    return true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is true that the spacings in each file are all self consistent for the subdirectory alphas\\\n",
      "It is true that the spacings in each file are all self consistent for the subdirectory deuterons\\\n",
      "It is true that the spacings in each file are all self consistent for the subdirectory gammas\\\n",
      "It is true that the spacings in each file are all self consistent for the subdirectory helions\\\n",
      "It is true that the spacings in each file are all self consistent for the subdirectory neutrons\\\n",
      "It is true that the spacings in each file are all self consistent for the subdirectory other\\\n",
      "It is true that the spacings in each file are all self consistent for the subdirectory protons\\\n"
     ]
    }
   ],
   "source": [
    "subdirs = [content * \"\\\\\" for content in readdir(data_dir) if isdir(data_dir * \"\\\\\" * content)]\n",
    "for subdir in subdirs\n",
    "    files = readdir(data_dir * subdir)\n",
    "    test = [check_spacings(data_dir * subdir * file) for file in files]\n",
    "    println(\"It is \", all(test),\n",
    "     \" that the spacings in each file are all self consistent for the subdirectory \", subdir)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I only need to retrieve the spacings of the data once for each file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It it faster to use 1 or loop for $n$ list comprehensions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read_exfor_file (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_field(field, input_array) = reduce(CatView, mappedarray(x->x[field], input_array))\n",
    "\n",
    "function read_exfor_file(file_path)\n",
    "    #Reads an EXFOR file and returns a dictionary of data\n",
    "    file_as_vector = readlines(file_path)\n",
    "    spacing_specifiers = [index for index in 1:length(file_as_vector) \n",
    "                        if check_line(file_as_vector[index], \"# Prj\")]\n",
    "    if spacing_specifiers == []\n",
    "        print(\"Error: no data found\")\n",
    "        return\n",
    "    end\n",
    "    dataset_rows = [tryparse(Int64, split(file_as_vector[index - 1], [' '])[end])\n",
    "                            for index in spacing_specifiers]\n",
    "    #Each dataset should have the same column names\n",
    "    spacing_dict = make_spacing_dict(file_as_vector[spacing_specifiers[1]], \n",
    "                    file_as_vector[spacing_specifiers[1] + 1])\n",
    "    #make an empty dataframe to fill up with the data\n",
    "    df = DataFrame([Vector{Union{Missing, Float64, String, SubString{String}}}(missing, sum(dataset_rows)) \n",
    "                   for _ in 1:length(keys(spacing_dict))], [key for key in keys(spacing_dict)])\n",
    "    _ = [df[:,key] = vec([[read_datum(file_as_vector[line_num][spacing_dict[key]]) \n",
    "                    for line_num in spacing_specifiers[index] + 2: \n",
    "                                    spacing_specifiers[index] + 1 + dataset_rows[index]]\n",
    "                                    for index in 1:length(spacing_specifiers)])\n",
    "                    for key in keys(spacing_dict)]           \n",
    "    return df\n",
    "    #df = DataFrame([Vector{Union{Missing, Float64, String, SubString{String}}}(missing, rows_in_combined_df) \n",
    "    #                for _ in 1:length(spacing_keys)], spacing_keys)\n",
    "    #Fill the dataframe with the data\n",
    "    #current_row = 1\n",
    "    #This for loop is probably the slowest part of the code. It alone takes over 50% of the runtime.  \n",
    "    #df = reduce(vcat,[DataFrame(read_dataset(spacing_specifiers[i], file_as_vector)) \n",
    "    #        for i in 1:length(spacing_specifiers)])\n",
    "    list_of_datasets = [read_dataset(spacing_specifiers[i], file_as_vector)\n",
    "                    for i in 1:length(spacing_specifiers)] \n",
    "    #final_dict = reduce(vcat, dict_list)\n",
    "    return list_of_datasets\n",
    "    df = get_field(\"Data\", [read_dataset(spacing_specifiers[i], file_as_vector)\n",
    "                             for i in 1:length(spacing_specifiers)])\n",
    "    #==\n",
    "    for spacing_specifier in spacing_specifiers\n",
    "        data_dict, rows_in_dataset = read_dataset(spacing_specifier, file_as_vector)\n",
    "        _ = [df[current_row:current_row+rows_in_dataset-1, key] = data_dict[key]\n",
    "            for key in keys(data_dict)]\n",
    "        current_row += rows_in_dataset\n",
    "    end\n",
    "    ==#\n",
    "    return df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving all of data at the correct spacings (without reorganizing it) takes 4 seconds for a large file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: index not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: index not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] (::var\"#132#139\"{Dict{Any, Any}, Vector{Int64}})(key::SubString{String})\n",
      "   @ Main .\\none:0\n",
      " [2] iterate\n",
      "   @ .\\generator.jl:47 [inlined]\n",
      " [3] collect(itr::Base.Generator{Base.KeySet{Any, Dict{Any, Any}}, var\"#132#139\"{Dict{Any, Any}, Vector{Int64}}})\n",
      "   @ Base .\\array.jl:724\n",
      " [4] read_exfor_file(file_path::String)\n",
      "   @ Main c:\\Users\\engin\\Documents\\GitHub\\Energy\\read_exfor.ipynb:20\n",
      " [5] top-level scope\n",
      "   @ c:\\Users\\engin\\Documents\\GitHub\\Energy\\read_exfor.ipynb:2"
     ]
    }
   ],
   "source": [
    "file_path = data_dir * \"alphas\\\\007_N_014.c4\"\n",
    "read_exfor_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe consider using CatViews? \n",
    "\n",
    "https://www.juliapackages.com/p/catviews \n",
    "\n",
    "https://stackoverflow.com/questions/46301279/julia-efficient-ways-to-vcat-n-arrays \n",
    "\n",
    "method5(json_in) = reduce(CatView, mappedarray(x->x[\"transactions\"], json_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200-element CatView{2, Float64}:\n",
       "  0.9951794404941244\n",
       " -0.710940746884212\n",
       "  0.46077682059309294\n",
       "  2.20655157813473\n",
       " -0.726121253635956\n",
       "  0.047923866801383386\n",
       "  0.31472713103841204\n",
       "  0.8359749511993977\n",
       "  0.8541259742661336\n",
       "  0.8118137456720229\n",
       "  ⋮\n",
       " -1.5251789978482457\n",
       " -0.8789687003101161\n",
       "  0.15513860791591735\n",
       "  0.3847956801629314\n",
       " -0.6254079611667328\n",
       " -0.08286596220103365\n",
       " -0.09567205982105349\n",
       "  0.8917107265925021\n",
       "  1.2570873129485147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = randn(10, 10);\n",
    "B = randn(10, 10);\n",
    "a = view(A, :);      # no copying\n",
    "b = view(B, :);      # no copying\n",
    "x = CatView(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Dict([\"cat\" => 1, \"dog\" => 2])\n",
    "length(keys(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch(\"new dimensions (1000, 1) must be consistent with array size 1\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"new dimensions (1000, 1) must be consistent with array size 1\")\n",
      "\n",
      "Stacktrace:\n",
      " [1] (::Base.var\"#throw_dmrsa#272\")(dims::Tuple{Int64, Int64}, len::Int64)\n",
      "   @ Base .\\reshapedarray.jl:41\n",
      " [2] reshape\n",
      "   @ .\\reshapedarray.jl:45 [inlined]\n",
      " [3] reshape(::Vector{UnitRange{Int64}}, ::Int64, ::Int64)\n",
      "   @ Base .\\reshapedarray.jl:116\n",
      " [4] top-level scope\n",
      "   @ c:\\Users\\engin\\Documents\\GitHub\\Energy\\read_exfor.ipynb:1"
     ]
    }
   ],
   "source": [
    "c = reshape([1:1000], 1000,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2 layer nested vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Vector{Int64}:\n",
       "   1\n",
       "   2\n",
       "   3\n",
       "   4\n",
       "   5\n",
       "   6\n",
       "   7\n",
       "   8\n",
       "   9\n",
       "  10\n",
       "   ⋮\n",
       "  20\n",
       "  30\n",
       "  40\n",
       "  50\n",
       "  60\n",
       "  70\n",
       "  80\n",
       "  90\n",
       " 100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [[m*n for n in 1:10] for m in 1:10]\n",
    "collect(Iterators.flatten(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 3 layer nested vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220-element Vector{Int64}:\n",
       "  1\n",
       "  1\n",
       "  2\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  4\n",
       "  ⋮\n",
       "  8\n",
       "  9\n",
       "  8\n",
       "  9\n",
       " 10\n",
       "  9\n",
       "  9\n",
       " 10\n",
       " 10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [[collect(m:n) for n in 1:10] for m in 1:10]\n",
    "collect(Iterators.flatten(collect.(Iterators.flatten.(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch(\"new dimensions (1000, 1) must be consistent with array size 1\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"new dimensions (1000, 1) must be consistent with array size 1\")\n",
      "\n",
      "Stacktrace:\n",
      " [1] (::Base.var\"#throw_dmrsa#272\")(dims::Tuple{Int64, Int64}, len::Int64)\n",
      "   @ Base .\\reshapedarray.jl:41\n",
      " [2] reshape\n",
      "   @ .\\reshapedarray.jl:45 [inlined]\n",
      " [3] reshape(::Vector{UnitRange{Int64}}, ::Int64, ::Int64)\n",
      "   @ Base .\\reshapedarray.jl:116\n",
      " [4] top-level scope\n",
      "   @ c:\\Users\\engin\\Documents\\GitHub\\Energy\\read_exfor.ipynb:2"
     ]
    }
   ],
   "source": [
    "d = vec([1:1000])\n",
    "reshape(d, 1000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "274785dbb282d05f4e0ae2ca70ac66f1fa0484127cd60ef9f4ac2dd997977034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
